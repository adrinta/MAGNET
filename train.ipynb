{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of train-glove.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eNCR6VcqUlc_"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phPvDF_WTeH9"
      },
      "source": [
        "#Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeHfYDNGoGka",
        "outputId": "a7cd38a6-7aad-439b-9779-432e01c56b9c"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import reuters\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import PorterStemmer, WordNetLemmatizer\n",
        "import re\n",
        "import string\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import hamming_loss, f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import pickle\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from models import MAGNET"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2EG3jePzrE9"
      },
      "source": [
        "with ZipFile('./glove.6B.zip', 'r') as file:\n",
        "   file.extractall(path='./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xLEwWKB6QtL"
      },
      "source": [
        "#Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O8H5bRTsHPL"
      },
      "source": [
        "#Build adjacency matrix based on Co-Occurencies label\n",
        "def buildAdjacencyCOOC(data_label):\n",
        "  adj = data_label.T.dot(data_label).astype('float')\n",
        "  for i in range(len(adj)):\n",
        "    adj[i] = adj[i] / adj[i,i]\n",
        "  \n",
        "  return torch.from_numpy(adj.astype('float32'))\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#Text cleaning function\n",
        "def preprocessingText(text, stop=stop):\n",
        "  text = text.lower() #text to lowercase\n",
        "  text = re.sub(r'&lt;', '', text) #remove '&lt;' tag\n",
        "  text = re.sub(r'<.*?>', '', text) #remove html\n",
        "  text = re.sub(r'[0-9]+', '', text) #remove number\n",
        "  text = \" \".join([word for word in text.split() if word not in stop]) #remove stopwords\n",
        "  text = re.sub(r'[^\\w\\s]', '', text) #remove punctiation\n",
        "  text = re.sub(r'[^\\x00-\\x7f]', '', text) #remove non ASCII strings\n",
        "  for c in ['\\r', '\\n', '\\t'] :\n",
        "    text = re.sub(c, ' ', text) #replace newline and tab with tabs\n",
        "  text = re.sub('\\s+', ' ', text) #replace multiple spaces with one space\n",
        "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "  return text\n",
        "\n",
        "#Load Word Representation Vector\n",
        "def loadWRVModel(File):\n",
        "    print(\"Loading Word Representation Vector Model\")\n",
        "    f = open(File,'r')\n",
        "    WRVModel = {}\n",
        "    for line in f:\n",
        "        splitLines = line.split()\n",
        "        word = splitLines[0]\n",
        "        try:\n",
        "          wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
        "        except:\n",
        "          print(splitLines[1:])\n",
        "          print(len(splitLines[1:]))\n",
        "          break\n",
        "        WRVModel[word] = wordEmbedding\n",
        "    print(len(WRVModel),\" words loaded!\")\n",
        "    return WRVModel\n",
        "\n",
        "\n",
        "def check_accuracy(model, label_embedding, X, y):\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out = model(X, label_embedding)\n",
        "    y_pred = torch.sigmoid(out.detach()).round().cpu()\n",
        "    f1score = f1_score(y, y_pred, average='micro')\n",
        "    hammingloss = hamming_loss(y, y_pred)\n",
        "  \n",
        "  return hammingloss, f1score\n",
        "\n",
        "def train(model,\n",
        "          X_train,\n",
        "          X_test,\n",
        "          label_embedding,\n",
        "          y_train,\n",
        "          y_test,\n",
        "          total_epoch=250,\n",
        "          batch_size=250,\n",
        "          learning_rate=0.001,\n",
        "          save_path='./model.pt',\n",
        "          state=None):\n",
        "  \n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  train_data = DataLoader(dataset(X_train, y_train), batch_size=batch_size)\n",
        "  X_test = X_test.to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  label_embedding= label_embedding.to(device)\n",
        "\n",
        "  if state:\n",
        "    state = torch.load(state)\n",
        "    model = model.load_state_dict(state['last_model'])\n",
        "    optimizer = optimizer.load_state_dict(state['optimizer'])\n",
        "  \n",
        "  else:\n",
        "    model = model.to(device)\n",
        "    state = dict()\n",
        "    state['microf1'] = []\n",
        "    state['hammingloss'] = []\n",
        "    state['val_hammingloss'] = []\n",
        "    state['val_microf1'] = []\n",
        "    state['epoch_time'] = []\n",
        "    \n",
        "  epoch = 1\n",
        "  \n",
        "  best_train = 0\n",
        "  best_val = 0\n",
        "  \n",
        "  while epoch <= total_epoch:\n",
        "    running_loss = 0\n",
        "    y_pred = []\n",
        "    epoch_time = 0\n",
        "    model.train()\n",
        "    for index, (X, y) in enumerate(train_data):\n",
        "      \n",
        "      t = time.time()\n",
        "\n",
        "      #forward\n",
        "      out = model(X.to(device), label_embedding)\n",
        "      loss = criterion(out, y.to(device))\n",
        "\n",
        "      #backward\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "      #update\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_time += time.time() - t\n",
        "      y_pred.append(torch.sigmoid(out.detach()).round().cpu())\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    y_pred = torch.vstack(y_pred)\n",
        "    f1score = f1_score(y_train, y_pred, average='micro')\n",
        "    hammingloss = hamming_loss(y_train, y_pred)\n",
        "    val_hamming, val_f1score = check_accuracy(model, label_embedding, X_test, y_test)\n",
        "\n",
        "    state['microf1'].append(f1score)\n",
        "    state['hammingloss'].append(hammingloss)\n",
        "    state['val_microf1'].append(val_f1score)\n",
        "    state['epoch_time'].append(epoch_time)\n",
        "    state['val_hammingloss'].append(val_hamming)\n",
        "\n",
        "    state['optimizer'] = optimizer.state_dict()\n",
        "    state['last_model'] = model.state_dict()\n",
        "    \n",
        "    \n",
        "    if(best_train < f1score):\n",
        "      state['model_best_train'] = copy.deepcopy(model.state_dict())\n",
        "      best_train = f1score\n",
        "      state['best_train'] = best_train\n",
        "    \n",
        "    if(best_val < val_f1score):\n",
        "      state['model_best_val'] = copy.deepcopy(model.state_dict())\n",
        "      best_val = val_f1score\n",
        "      state['best_val'] = best_val\n",
        "\n",
        "    torch.save(state, save_path)\n",
        "    print('epoch:{} loss:{:.5f} hamming_loss:{:.5f} micro_f1score:{:.5f} val_hamming_loss:{:.5f} val_micro_f1score:{:.5f}'.\n",
        "          format(epoch, running_loss, hammingloss, f1score, val_hamming, val_f1score))\n",
        "    epoch+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgZvCUaKY-xZ"
      },
      "source": [
        "#Load Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1PuvFDqT2Dv"
      },
      "source": [
        "## Load Raw Dataset (Reuters-21578)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD3nYHOGiwSQ",
        "outputId": "1b974b72-a52e-476f-fb97-ff1055ba59df"
      },
      "source": [
        "data_train = pd.read_pickle('./train.pickle')\n",
        "data_test = pd.read_pickle('./test.pickle')\n",
        "\n",
        "text_train = data_train.text.values\n",
        "text_test = data_test.text.values\n",
        "\n",
        "y_train = torch.from_numpy(np.vstack(data_train.onehot_label.values)).float()\n",
        "y_test = torch.from_numpy(np.vstack(data_test.onehot_label.values)).float()\n",
        "\n",
        "print('Train label shape {}'.format(y_train.shape))\n",
        "print('Test label shape {}'.format(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train label shape torch.Size([7769, 90])\n",
            "Test label shape torch.Size([3019, 90])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ-0B4vnZJpB"
      },
      "source": [
        "#Load MultilabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSJ0YUzGZTO-"
      },
      "source": [
        "with open('./multilabelbinarizer.pickle', 'rb') as file:\n",
        "  mlb = pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RminIPRXkY8"
      },
      "source": [
        "#Build Word Representation Vector Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3L7jlurXpjk",
        "outputId": "78d689d5-c4a2-4cd1-9998-b07f62a8fc57"
      },
      "source": [
        "WRVModel = loadWRVModel('./glove.6B.300d.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Word Representation Vector Model\n",
            "400000  words loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjxasUuZ4lRi"
      },
      "source": [
        "#Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq5ref4z4oCq"
      },
      "source": [
        "## Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIAPLVCmox6A",
        "outputId": "0c965d16-c384-479b-b9bc-fe5f775a3dbd"
      },
      "source": [
        "preprocessed_text_train = [preprocessingText(text) for text in text_train]\n",
        "preprocessed_text_test = [preprocessingText(text) for text in text_test]\n",
        "\n",
        "print('BEFORE CLEANING: {}'.format(text_train[0]))\n",
        "print('AFTER CLEANING: {}'.format(preprocessed_text_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BEFORE CLEANING: BAHIA COCOA REVIEW\n",
            "  Showers continued throughout the week in\n",
            "  the Bahia cocoa zone, alleviating the drought since early\n",
            "  January and improving prospects for the coming temporao,\n",
            "  although normal humidity levels have not been restored,\n",
            "  Comissaria Smith said in its weekly review.\n",
            "      The dry period means the temporao will be late this year.\n",
            "      Arrivals for the week ended February 22 were 155,221 bags\n",
            "  of 60 kilos making a cumulative total for the season of 5.93\n",
            "  mln against 5.81 at the same stage last year. Again it seems\n",
            "  that cocoa delivered earlier on consignment was included in the\n",
            "  arrivals figures.\n",
            "      Comissaria Smith said there is still some doubt as to how\n",
            "  much old crop cocoa is still available as harvesting has\n",
            "  practically come to an end. With total Bahia crop estimates\n",
            "  around 6.4 mln bags and sales standing at almost 6.2 mln there\n",
            "  are a few hundred thousand bags still in the hands of farmers,\n",
            "  middlemen, exporters and processors.\n",
            "      There are doubts as to how much of this cocoa would be fit\n",
            "  for export as shippers are now experiencing dificulties in\n",
            "  obtaining +Bahia superior+ certificates.\n",
            "      In view of the lower quality over recent weeks farmers have\n",
            "  sold a good part of their cocoa held on consignment.\n",
            "      Comissaria Smith said spot bean prices rose to 340 to 350\n",
            "  cruzados per arroba of 15 kilos.\n",
            "      Bean shippers were reluctant to offer nearby shipment and\n",
            "  only limited sales were booked for March shipment at 1,750 to\n",
            "  1,780 dlrs per tonne to ports to be named.\n",
            "      New crop sales were also light and all to open ports with\n",
            "  June/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\n",
            "  under New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\n",
            "  per tonne FOB.\n",
            "      Routine sales of butter were made. March/April sold at\n",
            "  4,340, 4,345 and 4,350 dlrs.\n",
            "      April/May butter went at 2.27 times New York May, June/July\n",
            "  at 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\n",
            "  2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\n",
            "  2.27 times New York Dec, Comissaria Smith said.\n",
            "      Destinations were the U.S., Covertible currency areas,\n",
            "  Uruguay and open ports.\n",
            "      Cake sales were registered at 785 to 995 dlrs for\n",
            "  March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\n",
            "  New York Dec for Oct/Dec.\n",
            "      Buyers were the U.S., Argentina, Uruguay and convertible\n",
            "  currency areas.\n",
            "      Liquor sales were limited with March/April selling at 2,325\n",
            "  and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\n",
            "  York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\n",
            "  Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\n",
            "  said.\n",
            "      Total Bahia sales are currently estimated at 6.13 mln bags\n",
            "  against the 1986/87 crop and 1.06 mln bags against the 1987/88\n",
            "  crop.\n",
            "      Final figures for the period to February 28 are expected to\n",
            "  be published by the Brazilian Cocoa Trade Commission after\n",
            "  carnival which ends midday on February 27.\n",
            "  \n",
            "\n",
            "\n",
            "AFTER CLEANING: bahia cocoa review shower continued throughout week bahia cocoa zone alleviating drought since early january improving prospect coming temporao although normal humidity level restored comissaria smith said weekly review dry period mean temporao late year arrival week ended february bag kilo making cumulative total season mln stage last year seems cocoa delivered earlier consignment included arrival figure comissaria smith said still doubt much old crop cocoa still available harvesting practically come end total bahia crop estimate around mln bag sale standing almost mln hundred thousand bag still hand farmer middleman exporter processor doubt much cocoa would fit export shipper experiencing dificulties obtaining bahia superior certificate view lower quality recent week farmer sold good part cocoa held consignment comissaria smith said spot bean price rose cruzados per arroba kilo bean shipper reluctant offer nearby shipment limited sale booked march shipment dlrs per tonne port named new crop sale also light open port junejuly going dlrs dlrs new york july augsept dlrs per tonne fob routine sale butter made marchapril sold dlrs aprilmay butter went time new york may junejuly dlrs augsept dlrs time new york sept octdec dlrs time new york dec comissaria smith said destination u covertible currency area uruguay open port cake sale registered dlrs marchapril dlrs may dlrs aug time new york dec octdec buyer u argentina uruguay convertible currency area liquor sale limited marchapril selling dlrs junejuly dlrs time new york july augsept dlrs time new york sept octdec time new york dec comissaria smith said total bahia sale currently estimated mln bag crop mln bag crop final figure period february expected published brazilian cocoa trade commission carnival end midday february\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6CbF8ci4yW2"
      },
      "source": [
        "##Text to Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8rCpqQC4OHq"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_text_train)\n",
        "\n",
        "sequences_text_train = tokenizer.texts_to_sequences(preprocessed_text_train)\n",
        "sequences_text_test = tokenizer.texts_to_sequences(preprocessed_text_test)\n",
        "\n",
        "X_train = torch.from_numpy(pad_sequences(sequences_text_train, maxlen=128))\n",
        "X_test = torch.from_numpy(pad_sequences(sequences_text_test, maxlen=128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZi_UZbg43uw"
      },
      "source": [
        "## Build Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7_sCNk3vQvD",
        "outputId": "8c88142d-9f69-4c25-89e6-980b8f1a5503"
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = torch.zeros(VOCAB_SIZE, 300)\n",
        "\n",
        "unk = 0\n",
        "for i in range(1, VOCAB_SIZE):\n",
        "  word = tokenizer.index_word[i]\n",
        "  if word in WRVModel.keys():\n",
        "    embedding_matrix[i] = torch.from_numpy(WRVModel[word]).float()\n",
        "  else:\n",
        "    unk +=1\n",
        "print('VOCAB_SIZE : {}'.format(VOCAB_SIZE))\n",
        "print('TOTAL OF UNKNOWN WORD : {}'.format(unk))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VOCAB_SIZE : 25306\n",
            "TOTAL OF UNKNOWN WORD : 6568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZHdtuWH6kiT"
      },
      "source": [
        "#Preparing Graph Attention Networks Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhxTcQ_u6sUc"
      },
      "source": [
        "## Label Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URGNwS1hP_fr",
        "outputId": "b045df30-e914-4f7f-f244-f625349c8a98"
      },
      "source": [
        "label_embedding = torch.zeros(90,300)\n",
        "\n",
        "for index, label in enumerate(mlb.classes_):\n",
        "  words = label.split('-')\n",
        "  num_of_words = len(words)\n",
        "\n",
        "  for sublabel in words:\n",
        "    if sublabel in WRVModel.keys():\n",
        "      label_embedding[index] +=  torch.from_numpy(WRVModel[sublabel])\n",
        "  label_embedding[index] = label_embedding[index]/num_of_words\n",
        "\n",
        "print(label_embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1796, -0.1051, -0.5564,  ..., -0.0633,  0.3732, -0.2873],\n",
            "        [ 0.1101,  0.4061,  0.2036,  ..., -0.1957, -0.4627,  0.6931],\n",
            "        [-0.3568, -0.1348,  0.0790,  ..., -0.0384,  0.2948,  0.1996],\n",
            "        ...,\n",
            "        [-0.1446,  0.0594, -0.1450,  ..., -0.0334,  0.1966,  0.4136],\n",
            "        [-0.5990, -0.3234, -0.2749,  ...,  0.6343,  0.5300,  0.0299],\n",
            "        [-0.4541, -0.1300, -0.5178,  ..., -1.1637, -0.2056, -0.3177]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PvT83CK66F-"
      },
      "source": [
        "##Adjacency Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhYLvfvTtZOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38130988-ab9b-4c3a-be03-4763e647d7cb"
      },
      "source": [
        "adjacency = buildAdjacencyCOOC(y_train.numpy())\n",
        "print(adjacency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0012],\n",
            "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0286],\n",
            "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
            "        [0.0952, 0.0476, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0dSc09x7lGm"
      },
      "source": [
        "#Preparing DataLoader and Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uoQlm38OS5S"
      },
      "source": [
        "## Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vePGeWyvocZ"
      },
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x  = x\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi7g_VSYOYny"
      },
      "source": [
        "##Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apRrzy6cWzN5"
      },
      "source": [
        "model = MAGNET(300, 250, adjacency, embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvY8vULwSbXB"
      },
      "source": [
        "#Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTh-66uJSnij"
      },
      "source": [
        "##Configure Save PATH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC2_KVL5SfG-"
      },
      "source": [
        "save_path = './train_result.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwv_OW8zSx7W"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XeYrWVgvYrV"
      },
      "source": [
        "train(model, X_train, X_test, label_embedding, y_train, y_test, save_path=save_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}